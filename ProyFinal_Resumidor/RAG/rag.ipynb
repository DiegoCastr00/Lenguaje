{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install langchain huggingface_hub tiktoken\n",
    "%pip -q install chromadb\n",
    "%pip -q install PyPDF2 pypdf sentence_transformers\n",
    "%pip -q install --upgrade together\n",
    "%pip -q install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khoss\\AppData\\Local\\Temp\\ipykernel_24688\\1057387006.py:32: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.7/migration/\n",
      "  extra = Extra.forbid\n"
     ]
    }
   ],
   "source": [
    "import together\n",
    "from typing import Any, Dict\n",
    "from pydantic import Extra\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"f8935229473a0d8a3f4709a9ef32533fe365c0cb215ba8c41413b5ca53a5c767\"\n",
    "\n",
    "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
    "    \"\"\"Together API key\"\"\"\n",
    "\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the API key is set.\"\"\"\n",
    "        api_key = get_from_dict_or_env(\n",
    "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
    "        )\n",
    "        values[\"together_api_key\"] = api_key\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of LLM.\"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint.\"\"\"\n",
    "        together.api_key = self.together_api_key\n",
    "        output = together.Complete.create(prompt,\n",
    "                                        model=self.model,\n",
    "                                        max_tokens=self.max_tokens,\n",
    "                                        temperature=self.temperature,\n",
    "                                        )\n",
    "        # print(\"Output:\", output)  # print the entire output\n",
    "        if 'choices' in output:\n",
    "            text = output['choices'][0]['text']\n",
    "            return text\n",
    "        else:\n",
    "            raise KeyError(\"The key 'choices' is not in the response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khoss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def create_embeddings(pdf):\n",
    "    pdf_reader = PdfReader(pdf)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "        )        \n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    knowledge_base = FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "    return knowledge_base\n",
    "\n",
    "\n",
    "pdf_obj = \"./prueba.pdf\"\n",
    "knowledge_base = create_embeddings(pdf_obj)\n",
    "retriever = knowledge_base.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "## Default LLaMA-2 prompt style\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
    "\n",
    "If the question is not directly related to the provided context, politely inform the user that the question is outside the context scope and cannot be answered accurately.\n",
    "\n",
    "Ensure that your answers are clear and concise, avoiding ambiguity or vague responses.\"\"\"\n",
    "\n",
    "instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "\n",
    "get_prompt(instruction, sys_prompt)\n",
    "\n",
    "llm = TogetherLLM(\n",
    "    model= \"togethercomputer/llama-2-7b-chat\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 1024\n",
    ")\n",
    "\n",
    "prompt_template = get_prompt(instruction, sys_prompt)\n",
    "\n",
    "llama_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": llama_prompt}\n",
    "\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "\n",
    "    from langchain.chains import RetrievalQA\n",
    "\n",
    "# create the chain to answer questions\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=retriever,\n",
    "                                       chain_type_kwargs=chain_type_kwargs,\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khoss\\AppData\\Local\\Temp\\ipykernel_24688\\1057387006.py:54: DeprecationWarning: Call to deprecated function create.\n",
      "  output = together.Complete.create(prompt,\n",
      "c:\\Users\\Khoss\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\together\\legacy\\complete.py:23: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
      "  warnings.warn(API_KEY_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  According to the provided text, the related research found that:\n",
      "\n",
      "1. A more robust approach to object recognition was demonstrated by Edelman, Intrator, and Poggio (1997) using\n",
      "a model of biological vision.\n",
      "2. The function of complex neurons in the primary visual cortex is to allow for matching and recognition of 3D\n",
      "objects from a range of viewpoints.\n",
      "3. The use of scale-space theory and the identification of peak s and ridges in scale space can provide a\n",
      "stable representation under scale change.\n",
      "4. Graph-based matching using wavelet coefficients provides more distinctive feature descriptors.\n",
      "5. The problem of identifying an appropriate and consistent scale for feature detection has been studied in\n",
      "depth.\n",
      "6. The local feature approach can easily incorporate other useful properties, such as color, motion, figure-\n",
      "ground discrimination, region shape descriptors, and stereo depth cues.\n"
     ]
    }
   ],
   "source": [
    "query = \"What was found with the related research?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
