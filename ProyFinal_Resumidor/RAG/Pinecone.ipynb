{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import together\n",
    "from typing import Any\n",
    "from langchain.llms.base import LLM\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NO TOCAR!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOGETHER_API_KEY\"] = \"f8935229473a0d8a3f4709a9ef32533fe365c0cb215ba8c41413b5ca53a5c767\"\n",
    "\n",
    "class TogetherLLM(LLM):\n",
    "    model: str = \"togethercomputer/llama-2-7b-chat\"\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 1024\n",
    "    together_api_key: str = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "    \n",
    "    class Config:\n",
    "        extra = 'forbid'\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"together\"\n",
    "    def _call(self, prompt: str, **kwargs: Any) -> str:\n",
    "        if not self.together_api_key:\n",
    "            raise ValueError(\"API key is not set.\")\n",
    "        together.api_key = self.together_api_key\n",
    "        output = together.Complete.create(prompt,\n",
    "                                        model=self.model,\n",
    "                                        max_tokens=self.max_tokens,\n",
    "                                        temperature=self.temperature)\n",
    "        print(f\"modelo: {self.model}\")\n",
    "        if 'choices' in output:\n",
    "            return output['choices'][0]['text']\n",
    "        else:\n",
    "            raise KeyError(\"The key 'choices' is not in the response.\")\n",
    "\n",
    "# # Funcion para traduccion a ingles\n",
    "# from googletrans import Translator\n",
    "# import googletrans\n",
    "# translator = Translator()\n",
    "\n",
    "# def traducir(texto_original):\n",
    "#     try :\n",
    "#         origen = translator.detect(texto_original).lang\n",
    "#         # Si el texto esta en otro idioma que no sea ingles lo traduce\n",
    "#         if origen != \"en\":\n",
    "#             #print(f'Traduccion de {origen} a en')\n",
    "#             traduccion = translator.translate(texto_original, dest=\"en\", src=origen).text\n",
    "#             return traduccion\n",
    "#         # En caso contrario devuelve el texto original ya que esta en ingles\n",
    "#         else:\n",
    "#             return texto_original\n",
    "#     except:\n",
    "#         return texto_original\n",
    "    \n",
    "# # Funcion para traduccion a otro idioma\n",
    "# def traducir_ingles(texto_ingles, idioma_destino):\n",
    "#     for abreviatura, nombre_idioma in googletrans.LANGUAGES.items():\n",
    "#         if nombre_idioma == idioma_destino:\n",
    "#             destino = abreviatura\n",
    "#     traduccion = translator.translate(texto_ingles, dest=destino, src=\"en\").text\n",
    "#     return traduccion\n",
    "\n",
    "# def dividir_texto(texto, max_caracteres):\n",
    "#     textos_divididos = []\n",
    "#     texto_actual = ''\n",
    "#     caracteres_actuales = 0\n",
    "\n",
    "#     oraciones = texto.split('.')\n",
    "\n",
    "#     for oracion in oraciones:\n",
    "#         caracteres_actuales += len(oracion) + 1\n",
    "\n",
    "#         if caracteres_actuales <= max_caracteres:\n",
    "#             texto_actual += oracion + '.'\n",
    "#         else:\n",
    "#             textos_divididos.append(texto_actual.strip())\n",
    "#             texto_actual = oracion + '.'\n",
    "#             caracteres_actuales = len(oracion) + 1\n",
    "\n",
    "#     textos_divididos.append(texto_actual.strip())\n",
    "\n",
    "#     return textos_divididos\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "sys_prompt = \"\"\"You are a helpful, respectful, and honest assistant. Always provide the most helpful and accurate answer using only the contextual text provided. Do not add any information that is not in the context.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "1. **Relevance**: Only answer questions based on the provided context. Do not use outside knowledge.\n",
    "2. **Honesty**: If a question does not make sense or is factually incoherent, explain why instead of providing incorrect information.\n",
    "3. **Integrity**: If you don't know the answer based on the context, do not share false information.\n",
    "4. **Scope**: If the question is outside the context, inform the user politely that it cannot be answered accurately.\n",
    "5. **Clarity**: Ensure your answers are clear and concise, avoiding ambiguity or vagueness.\n",
    "6. **Finality**: Answer the question directly and do not include any additional text after the answer.\n",
    "\n",
    "Example of handling an irrelevant question:\n",
    "- \"I'm sorry, but this question is outside the provided context and I cannot answer it accurately.\"\n",
    "\n",
    "Example of handling a nonsensical question:\n",
    "- \"The question seems factually incoherent, so I cannot provide an accurate answer.\"\n",
    "\n",
    "Ensure that your answers are clear and concise, avoiding ambiguity or vague responses.\"\"\"\n",
    "\n",
    "instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt_template = get_prompt(instruction, sys_prompt)\n",
    "llama_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": llama_prompt}\n",
    "\n",
    "def modelo_llm(modelo):\n",
    "    return TogetherLLM(\n",
    "        model= modelo,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamano de documentos: 38\n",
      "tamano de chunks: 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dante/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/dante/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_documents(directory):\n",
    "    documentPDF = PyPDFDirectoryLoader(directory)\n",
    "    documents = documentPDF.load()\n",
    "    return documents\n",
    "\n",
    "def chunk_data(docs, chunk_size=800, overlap=100):\n",
    "    text_spliter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap\n",
    "        )\n",
    "    return text_spliter.split_documents(docs)\n",
    "\n",
    "\n",
    "documents = load_documents('documents/')\n",
    "print(f\"tamano de documentos: {len(documents)}\")\n",
    "chunks = chunk_data(documents)\n",
    "print(f\"tamano de chunks: {len(chunks)}\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea usar Pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "# from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# os.environ['PINECONE_API_KEY'] = '57210699-f119-4a04-9224-30521bbd7dc3'\n",
    "\n",
    "# index_name = \"chatify-384\"\n",
    "\n",
    "# pinecone = PineconeVectorStore.from_documents(\n",
    "#     chunks, embeddings, index_name=index_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'db'\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=chunks, \n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = \"togethercomputer/llama-2-7b-chat\"\n",
    "llm = modelo_llm(modelo)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    retriever=retriever,\n",
    "                                    return_source_documents=True,\n",
    "                                    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nFuentes:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        # Acceder a la metadata para obtener la p√°gina\n",
    "        page_number = source.metadata['page']\n",
    "        print(f\"Fuente: {source.metadata['source']}, Pagina: {page_number}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56196/579592326.py:18: DeprecationWarning: Call to deprecated function create.\n",
      "  output = together.Complete.create(prompt,\n",
      "/home/dante/.local/lib/python3.10/site-packages/together/legacy/complete.py:23: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
      "  warnings.warn(API_KEY_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo: togethercomputer/llama-2-7b-chat\n",
      "  Based on the provided context, the SS refers to the Schutzstaffel, which was a paramilitary organization that was part of the Nazi Party in Germany. The SS was responsible for various tasks, including security, identification of ethnic origins, demographic policy, and intelligence gathering. They also controlled the German police forces and the concentration camp system, and were involved in the reorganization of Eastern Europe and the Soviet Union under Nazi occupation.\n",
      "\n",
      "I hope this answer is helpful and accurate based on the context provided. Please let me know if you have any further questions.\n",
      "\n",
      "\n",
      "Sources:\n",
      "Source: documents/historia.pdf, Page: 3\n",
      "Source: documents/historia.pdf, Page: 2\n",
      "Source: documents/historia.pdf, Page: 7\n"
     ]
    }
   ],
   "source": [
    "query = \"Que es la SS\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conversation_chat(query,chain):\n",
    "#     #result =chain({\"question\":query, \"chat_history\": history})\n",
    "#     user_question_eng = traducir(query)\n",
    "#     llm_response = chain.invoke(user_question_eng)\n",
    "#     llm_response = traducir_ingles(llm_response['result'], \"spanish\")\n",
    "#     return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_obj = \"./historia.pdf\"\n",
    "# knowledge_base = create_embeddings(pdf_obj)\n",
    "\n",
    "# retriever = knowledge_base.as_retriever(search_kwargs={\"k\": 3})\n",
    "# modelo = \"togethercomputer/llama-2-7b-chat\"\n",
    "# llm = modelo_llm(modelo)\n",
    "# qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "#                                     chain_type=\"stuff\",\n",
    "#                                     retriever=retriever,\n",
    "#                                     chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "# user_question = \"De que trata el documento?\"\n",
    "# conversation_chat(user_question, qa_chain )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
