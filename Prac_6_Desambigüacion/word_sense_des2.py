# -*- coding: utf-8 -*-
"""Word_Sense_des2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bYXNNsoPVBljH7rrWMDmn2cd4ARnQCaM
"""

# !pip install spacy

# !pip install word2vec

# Elementos a utilizar
import nltk

nltk.download("webtext")
from nltk.corpus import webtext

nltk.download("stopwords")

print("the data contained in the webtext is :", webtext.fileids())

# utilizar la información del webchat de firefox
firefox = webtext.raw("firefox.txt")
print(firefox[:500])

firefox_sents = firefox.split("\n")
print(
    "el número de frases convertidas a partir del fichero de datos brutos es de:",
    len(firefox_sents),
)

# limpieza de los datos
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

Stopwords = stopwords.words("english")
import re

charfilter = re.compile("[a-zA-Z]+")


# tokenizar las palabras
def simple_filter(sent):
    # convertir todos los tokens a minúsculas:
    words = sent.split()
    word_lower = []
    for word in words:
        word_lower.append(word.lower())
    # eliminar todas las stopwords
    # eliminar caracteres
    tokens = list(filter(lambda token: charfilter.match(token), word_lower))
    # stemming de las palabras
    ntokens = []
    for word in tokens:
        ntokens.append(PorterStemmer().stem(word))
    return " ".join(tokens)


# convertir todos los datos de bryant a tokens utilizando nuestra función simple tokenizer
sentences = []
for sent in firefox_sents:
    tokens = simple_filter(sent)
    if len(tokens) > 0:
        sentences.append(tokens)

# Sense2Vec
# usar Spacy para calcular las etiquetas de POS de cada palabra
import spacy

nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

docs = []
count = 0
for item in sentences:
    docs.append(nlp(item))
    count += 1
sense_corpus = [[x.text + "_" + x.pos_ for x in y] for y in docs]

print(sense_corpus[:5])

# Ahora, utilizar el modelo word2vec en el corpus de sentidos creado
from gensim.test.utils import common_texts
from gensim.models import Word2Vec

# Los modelos colocados como comentarios son distintas formas de generar el modelado
model_skipgram = Word2Vec(
    sense_corpus, min_count=1, vector_size=50, workers=3, window=5, sg=1
)
# model_w2v = Word2Vec(sense_corpus,vector_size=200)
# model_skipgram  = Word2Vec(sentences=sense_corpus, vector_size=100, window=5, min_count=1, workers=4)

model_skipgram

import warnings

warnings.filterwarnings("ignore")

# Ahora, utilizar el modelo word2vec en el corpus de sentidos creado
from gensim.models import Word2Vec

model_skipgram = Word2Vec(
    sense_corpus, min_count=1, vector_size=50, workers=3, window=5, sg=1
)

# Configura los hiperparámetros del modelo
model = Word2Vec(
    sentences=sense_corpus, vector_size=100, window=5, min_count=1, sg=0, workers=4
)

# Entrena el modelo
model.train(sense_corpus, total_examples=model.corpus_count, epochs=100)

"""Observar la similitud entre palabras, web o internet por ejemplo"""

similar_words = model.wv.most_similar("web_NOUN", "internet_NOUN")
print(similar_words)

similar_words = model.wv.most_similar("web_NOUN", topn=3)
print(similar_words)

# Suponiendo que tienes un modelo Word2Vec entrenado con skip-gram
from gensim.models import Word2Vec

# Acceder al vocabulario
vocab = model_skipgram.wv.key_to_index

# Verificar si una palabra está en el vocabulario
if "web_NOUN" in vocab:
    print("La palabra 'web_NOUN' está en el vocabulario.")
else:
    print("La palabra 'ejemplo' no está en el vocabulario.")

# Obtener el vector de una palabra
vector = model_skipgram.wv["web_NOUN"]
print("Vector de la palabra 'ejemplo':", vector)

import numpy as np
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Suponiendo que se tiene un modelo Word2Vec entrenado
from gensim.models import Word2Vec

# Cargar o entrenar el modelo

# Obtener los vectores de palabras y sus etiquetas
words = []
vectors = []
for word in model_skipgram.wv.key_to_index:
    words.append(word)
    vectors.append(model_skipgram.wv[word])

# Realizar reducción de dimensionalidad con PCA
pca = PCA(n_components=2)
vectors_2d = pca.fit_transform(vectors)

# Alternativamente, puedes usar t-SNE para una mejor visualización
# t-SNE suele funcionar mejor que PCA para visualizar en 2D
# t-SNE puede tardar más en ejecutarse
# tsne = TSNE(n_components=2)
# vectors_2d = tsne.fit_transform(vectors)

# Crear un gráfico de dispersión para visualizar los vectores de palabras
plt.figure(figsize=(10, 10))
plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])

# Etiquetar algunos puntos en el gráfico para palabras específicas
for i, word in enumerate(words):
    if i % 50 == 0:
        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]))

plt.title("Visualización de Vectores de Palabras")
plt.show()

import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Suponiendo que tienes un modelo Word2Vec entrenado
from gensim.models import Word2Vec

# Cargar o entrenar el modelo

# Obtener los vectores de palabras y sus etiquetas
words = []
vectors = []
for word in model_skipgram.wv.key_to_index:
    words.append(word)
    vectors.append(model_skipgram.wv[word])

# Convertir la lista de listas a un arreglo NumPy
vectors = np.array(vectors)

# Realizar reducción de dimensionalidad con t-SNE
tsne = TSNE(n_components=2, random_state=42)
vectors_2d = tsne.fit_transform(vectors)

# Crear un gráfico de dispersión para visualizar los vectores de palabras
plt.figure(figsize=(10, 10))
plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])

# Etiquetar algunos puntos en el gráfico para palabras específicas
for i, word in enumerate(words):
    if i % 50 == 0:
        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]))

plt.title("Visualización de Vectores de Palabras con t-SNE")
plt.show()
